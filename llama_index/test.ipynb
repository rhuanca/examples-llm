{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renan/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 15/15 [00:00<00:00, 348.78it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:02<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7f64c2bd0640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x7f64c2bd26b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-vocabulary object detection refers to the task of detecting objects beyond the predefined categories. Traditional object detection methods are limited to detecting objects from a fixed vocabulary, but open-vocabulary object detection aims to detect and recognize novel objects that are not part of the predefined categories. This approach allows for greater flexibility and generalization in detecting objects in various scenarios and domains.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 7a487229-9c2d-4628-ab32-bcb98957b8d4\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 349f21c4-a67a-4c0d-a54e-0c297cae28a2\n",
      "Similarity: 0.8528921340113887\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods can only detect objects within a fixed vocabulary of\n",
      "predefined categories. However, open-vocabulary object detection aims\n",
      "to detect and recognize novel objects that are not part of the\n",
      "predefined categories. This allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 7a487229-9c2d-4628-ab32-bcb98957b8d4\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 349f21c4-a67a-4c0d-a54e-0c297cae28a2\n",
      "Similarity: 0.8528921340113887\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 24055a2b-8fdd-4968-b80d-2e684b097987\n",
      "Similarity: 0.8476892038587177\n",
      "Text: DetectorText Encoder(a) Traditional Object Detector(b)\n",
      "PreivousOpen-V ocabulary Detector(c) YOLO-WorldObject\n",
      "DetectorFixedVocabulary Text EncoderLarge Detector Text\n",
      "EncoderLightweight DetectorOfflineVocabulary\n",
      "UserUserOnlineVocabularyRe-parameterize UserTrain-onlyFigure 2.\n",
      "Comparison with Detection Paradigms. (a) Traditional Object Detector :\n",
      "Th...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: f8b1c95d-2197-425f-8ab7-cfe27e014893\n",
      "Similarity: 0.8372836395798543\n",
      "Text: {men, women, boy, girl}{golden dog, black dog, spotted\n",
      "dog}{elephant, ear, leg, trunk, ivory}{grass, sky, zebra, trunk,\n",
      "tree}Figure 6. Visualization Results on User’s Vocabulary. We define\n",
      "the custom vocabulary for each input image and YOLO-World can detect\n",
      "the accurate regions according to the vocabulary. Images are obtained\n",
      "from COCO val2017 ....\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.85)\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 7a487229-9c2d-4628-ab32-bcb98957b8d4\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 349f21c4-a67a-4c0d-a54e-0c297cae28a2\n",
      "Similarity: 0.8528921340113887\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part - Persist index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. It aims to detect\n",
      "and recognize novel objects that are not part of the limited dataset\n",
      "and vocabulary used for training. This approach allows for greater\n",
      "flexibility and generalization in detecting objects in open scenarios\n",
      "and different domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 763b56dc-c00a-4444-9c0d-3060c0d60ce0\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: dbb3abd3-5396-4bea-8ec1-0ebf5451bbb7\n",
      "Similarity: 0.8530501352776704\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "\n",
    "PERSISTENCE_DIR = \"./storage\"\n",
    "\n",
    "# If the index has not been persisted yet, create it and persist it\n",
    "if not os.path.exists(PERSISTENCE_DIR):\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    index.storage_context.persist(PERSISTENCE_DIR)\n",
    "# If the index has been persisted, load it from storage    \n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSISTENCE_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Create a query engine from the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is open-vocabulary object dection?\")\n",
    "\n",
    "pprint_response(response, show_source=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
