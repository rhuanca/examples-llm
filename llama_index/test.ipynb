{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/llms/utils.py:29\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     28\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:381\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/indices/base.py:98\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create index from documents.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m storage_context \u001b[38;5;129;01mor\u001b[39;00m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[0;32m---> 98\u001b[0m service_context \u001b[38;5;241m=\u001b[39m service_context \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m docstore \u001b[38;5;241m=\u001b[39m storage_context\u001b[38;5;241m.\u001b[39mdocstore\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m service_context\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_construction\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/service_context.py:178\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_predictor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMPredictor is deprecated, please use LLM instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mLLMPredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpydantic_program_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_program_mode\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm_predictor, LLMPredictor):\n\u001b[1;32m    182\u001b[0m     llm_predictor\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:109\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     llm: Optional[LLMType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     pydantic_program_mode: PydanticProgramMode \u001b[38;5;241m=\u001b[39m PydanticProgramMode\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback_manager:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n",
      "File \u001b[0;32m~/src-mydev/llm/llamaindex/venv/lib/python3.10/site-packages/llama_index/llms/utils.py:31\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m     29\u001b[0m         validate_openai_api_key(llm\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     42\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7ff953ecc5e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x7ff953ecf790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-vocabulary object detection refers to the task of detecting objects beyond the predefined categories. Traditional object detection methods are limited to detecting objects from a fixed vocabulary, but open-vocabulary object detection aims to detect and recognize novel objects that are not part of the predefined categories. This approach allows for greater flexibility and generalization in detecting objects in various scenarios and domains.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 84903d94-42b8-4fe0-b8f1-231ef4c88819\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 80678e33-cad4-4879-bd27-bd5008d01dee\n",
      "Similarity: 0.8530501352776704\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods can only detect objects within a fixed vocabulary of\n",
      "pre-defined categories. However, open-vocabulary object detection aims\n",
      "to detect objects that are not limited to these predefined categories.\n",
      "It allows for the detection and recognition of novel objects that may\n",
      "not have been seen during training. This is achieved by training\n",
      "detectors on a larger and more diverse dataset that includes a wider\n",
      "range of object categories. Open-vocabulary object detection is\n",
      "important for handling real-world scenarios where the objects to be\n",
      "detected may not be known in advance.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 84903d94-42b8-4fe0-b8f1-231ef4c88819\n",
      "Similarity: 0.854910315062603\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 80678e33-cad4-4879-bd27-bd5008d01dee\n",
      "Similarity: 0.8523339288059143\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 98bef1b3-8664-40ce-a6aa-0a15b5570d48\n",
      "Similarity: 0.8473455757796476\n",
      "Text: DetectorText Encoder(a) Traditional Object Detector(b)\n",
      "PreivousOpen-V ocabulary Detector(c) YOLO-WorldObject\n",
      "DetectorFixedVocabulary Text EncoderLarge Detector Text\n",
      "EncoderLightweight DetectorOfflineVocabulary\n",
      "UserUserOnlineVocabularyRe-parameterize UserTrain-onlyFigure 2.\n",
      "Comparison with Detection Paradigms. (a) Traditional Object Detector :\n",
      "Th...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 9a93f2d0-188d-438c-98e1-a0533a17e5b2\n",
      "Similarity: 0.8369937289904054\n",
      "Text: {men, women, boy, girl}{golden dog, black dog, spotted\n",
      "dog}{elephant, ear, leg, trunk, ivory}{grass, sky, zebra, trunk,\n",
      "tree}Figure 6. Visualization Results on User’s Vocabulary. We define\n",
      "the custom vocabulary for each input image and YOLO-World can detect\n",
      "the accurate regions according to the vocabulary. Images are obtained\n",
      "from COCO val2017 ....\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.85)\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, node_postprocessors=[postprocessor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. It aims to detect\n",
      "and recognize novel objects that may not be present in the training\n",
      "dataset or vocabulary. This approach allows for greater flexibility\n",
      "and generalization in object detection, as it is not limited to a\n",
      "fixed set of object categories.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 84903d94-42b8-4fe0-b8f1-231ef4c88819\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 80678e33-cad4-4879-bd27-bd5008d01dee\n",
      "Similarity: 0.8530501352776704\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is open-vocabulary object dection?\")\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part - Persist index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Open-vocabulary object detection refers to the task of\n",
      "detecting objects beyond the predefined categories. Traditional object\n",
      "detection methods are limited to detecting objects from a fixed\n",
      "vocabulary, but open-vocabulary object detection aims to detect and\n",
      "recognize novel objects that are not part of the predefined\n",
      "categories. This approach allows for greater flexibility and\n",
      "generalization in detecting objects in various scenarios and domains.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 763b56dc-c00a-4444-9c0d-3060c0d60ce0\n",
      "Similarity: 0.8554250535022178\n",
      "Text: During the past decades, the methods for traditional object de-\n",
      "tection can be simply categorized into three groups, i.e., region-\n",
      "based methods, pixel-based methods, and query- based methods. The\n",
      "region-based methods [10, 11, 15, 26, 41], such as Faster R-CNN [41],\n",
      "adopt a two-stage frame- work for proposal generation [41] and RoI-\n",
      "wise (Region- ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: dbb3abd3-5396-4bea-8ec1-0ebf5451bbb7\n",
      "Similarity: 0.8530501352776704\n",
      "Text: YOLO-World: Real-Time Open-Vocabulary Object Detection Tianheng\n",
      "Cheng3,2,∗, Lin Song1,∗,✉, Yixiao Ge1,2,†, Wenyu Liu3, Xinggang\n",
      "Wang3,✉, Ying Shan1,2 ∗equal contribution†project lead✉corresponding\n",
      "author 1Tencent AI Lab2ARC Lab, Tencent PCG 3School of EIC, Huazhong\n",
      "University of Science & Technology Code & Models: YOLO-World Abstract\n",
      "The You Onl...\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "\n",
    "PERSISTENCE_DIR = \"./storage\"\n",
    "\n",
    "# If the index has not been persisted yet, create it and persist it\n",
    "if not os.path.exists(PERSISTENCE_DIR):\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    index.storage_context.persist(PERSISTENCE_DIR)\n",
    "# If the index has been persisted, load it from storage    \n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSISTENCE_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Create a query engine from the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is open-vocabulary object dection?\")\n",
    "\n",
    "pprint_response(response, show_source=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
